{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Gradients using Captum Insights\n",
    "https://captum.ai/docs/captum_insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Installation\n",
    "* first install with conda : conda install -c pytorch captum \n",
    "* download captum locally in this folder\n",
    "* in ```captum.insights.attr_vis.features```, in class ```ImageFeature```, method ```visualize```, add this snippet of code at the start of the method: \n",
    "   ```\n",
    "        class UnNormalize(object):\n",
    "            def __init__(self, mean, std):\n",
    "                self.mean = mean\n",
    "                self.std = std\n",
    "\n",
    "            def __call__(self, tensor):\n",
    "                for t, m, s in zip(tensor, self.mean, self.std):\n",
    "                    t.mul_(s).add_(m)\n",
    "                return tensor\n",
    "                \n",
    "        unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        data = unorm(data)\n",
    "        print('data is unormalized')\n",
    "   ```\n",
    "    \n",
    "    This code is here so that visualization of integrated gradients is done with the image before the normalization from ImageNet\n",
    "    \n",
    "* then install captum locally from notebooks/captum/ : pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import pkg_resources\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "from captum.insights import AttributionVisualizer, Batch\n",
    "from captum.insights.attr_vis.features import ImageFeature\n",
    "\n",
    "from src.database import Database\n",
    "from src.protocol import Protocol \n",
    "from src import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose expert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp = 'TDP-43'\n",
    "classifier = 'als'\n",
    "channels = ['TDP-43']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'als': 'control_als_untreated',\n",
    "    'osmotic': 'control_untreated_osmotic',\n",
    "    'heat': 'control_untreated_heat',\n",
    "    'oxidative': 'control_untreated_oxidative',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: ['control', 'als']\n",
      "conditions: ['untreated']\n",
      "rbp: TDP-43\n"
     ]
    }
   ],
   "source": [
    "protocol_name = f'{classifiers[classifier]}_{rbp}'\n",
    "protocol = Protocol.from_name(protocol_name)\n",
    "print(protocol)\n",
    "\n",
    "if classifier == 'als': \n",
    "    classes = ['healthy', 'als']\n",
    "    classification = 'als'\n",
    "else:\n",
    "    classes = ['untreated','stress']\n",
    "    classification = 'stress'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for classification classes and pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes():\n",
    "    return classes\n",
    "\n",
    "\n",
    "def get_pretrained_model():\n",
    "    model = models.mobilenet_v2(pretrained=True)\n",
    "    for f in model.features: \n",
    "        f.requires_grad = False\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "    model.load_state_dict(torch.load(f'../models/{classifier}_models/state_dict_{protocol_name}_{\"_\".join(channels)}_fold_0.pt', map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def baseline_func(input):\n",
    "    return input * 0\n",
    "\n",
    "\n",
    "def formatted_data_iter():\n",
    "    database=Database()\n",
    "    config = utils.get_config('notebook')\n",
    "    train, test = database.cross_validation(classification, protocol, fold=0)\n",
    "    \n",
    "    train_loader = utils.create_train_dataloader(config, database, train, classification, protocol, channels, fold=0)\n",
    "    test_loader, test_dataset = utils.create_test_dataloader(config, database, test, classification, protocol, channels, fold=0)\n",
    "\n",
    "    dataloader = iter(test_loader)\n",
    "    \n",
    "    # select 10 images of each class \n",
    "    images = list()\n",
    "    labels = list()\n",
    "    n_images = dict()\n",
    "    n_images[classes[0]] = 0\n",
    "    n_images[classes[1]] = 0\n",
    "    limit_images = 10\n",
    "\n",
    "    print(len(dataloader))\n",
    "    while n_images[classes[0]] != limit_images or n_images[classes[1]] != limit_images:\n",
    "        image, label, _ = dataloader.next()    \n",
    "        while image==None: \n",
    "            print('image is None')\n",
    "            image, label, _ = dataloader.next()  \n",
    "\n",
    "        output = model(image)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "        predicted = classes[pred_label_idx.item()]\n",
    "        output = output.cpu().detach()\n",
    "        if prediction_score > 0.95 and n_images[predicted] < limit_images:\n",
    "#         if prediction_score > 0.4 and prediction_score < 0.6 and n_images[predicted] < limit_images:\n",
    "            n_images[predicted] += 1\n",
    "            image.requires_grad = True\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "            \n",
    "    images = torch.cat(images)\n",
    "    labels = torch.cat(labels)\n",
    "    yield Batch(inputs=images, labels=labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the visualizer and render inside notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_pretrained_model()\n",
    "visualizer = AttributionVisualizer(\n",
    "    models=[model],\n",
    "    score_func=lambda o: F.softmax(o, 1),\n",
    "    classes=get_classes(),\n",
    "    features=[\n",
    "        ImageFeature(\n",
    "            \"Photo\",\n",
    "            baseline_transforms=[baseline_func],\n",
    "            input_transforms= [],\n",
    "        )\n",
    "    ],\n",
    "    dataset=formatted_data_iter(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2967e0435fcb4806a26d8989dcc27b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CaptumInsights(insights_config={'classes': ['healthy', 'als'], 'methods': ['Deconvolution', 'Deep Lift', 'Guidâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7beae76d43a4f14b03a380e6de09dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualizer.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
